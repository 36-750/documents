* Divide and Conquer

  Suppose we want to write some code that can solve a particular problem --
  calculate some quantity, select data meeting certain requirements, whatever.
  The code to solve the problem is intimidating, involving big loops over lots
  of data or combinations or permutations or options.

  Suppose, however, that:

  1. The problem would be very easy to solve if there were only one or two data
     points (or options, or whichever).
  2. If you had the solution to two or more problems, it would be very easy to
     combine them to make the solution of a bigger problem.


  When a problem has these features, we can /divide and conquer/ to solve it.

  Under this strategy we solve a problem *recursively*. It has three main steps:

  1. *Divide* the problem into /smaller subproblems of the same type/.
  2. *Conquer* the subproblems:
     - Small enough subproblems can be solved directly.
       This is called the /base case/.
     - Otherwise, solve the subproblem /recursively/ -- using the
       original algorithm. This is called a /recursive case/.
  3. *Combine* the subproblem solutions into a solution for the
     original problem.

** Introductory Example

   Problem: Given a monetary value ~amount~ in cents and a set of coin
   denominations ~coins~, also in cents, generate /all/ the distinct ways to
   make change for the former in terms of the latter.

   Write this as a function ~change_for~ (or ~change-for~ or ~changeFor~
   depending on your language conventions). Also write a function
   ~best_change_for~, with the same arguments, that gives the way
   to make change with the smallest number of coins required.

   The function ~change_for~ should return a list of vectors, where each vector
   contains the coins used to make change. The returned list should
   contain no repeats (accounting for order). The function ~best_change_for~
   should return any single way of making change (a vector of denominations)
   of minimal size.

   For example:

   #+BEGIN_EXAMPLE
     change_for(17, [1, 5, 10, 25])

     # should return these results:
     [[10, 5, 1, 1],
      [5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
      [10, 1, 1, 1, 1, 1, 1, 1],
      [5, 5, 1, 1, 1, 1, 1, 1, 1],
      [5, 5, 5, 1, 1]]
   #+END_EXAMPLE

*** Discussion Questions
    + What are the subproblems here?
    + What subproblems are in the base case?
    + What is the recursive structure of this problem?
    + How are the subproblem results combined?
    + How can you ensure that there are no repeats in the generated list?
    + What is the interface for the function ~change-for~?
      #+begin_example
        change_for(amount, coins, current=c())
      #+end_example

*** Constructing Tests

    Generate some test ideas with a partner
    + All ways of making change should contain positive values only
    + All coins given are positive integers (assertion or test?)
    + Amounts should be equal to the sum of *each* way of making change
    + Do the right thing when we can't make change
    + Returned list should contain a singleton of a coin if the
      amount exactly matches that coin
    + Do the right thing when the amount is zero (or negative)

    #+begin_src R
      library(testthat)
      library(assertthat)
      context("Computing change, some sample tests")

      equals_any_order <- function(expected,
                                   key=function(v) paste(v, sep=",", collapse=""),
                                   label=NULL,
                                   ...) {
          function(x) expect_equal(x[order(sapply(x, key))],
                                   expected[order(sapply(expected, key))],
                                   ..., expected.label=label)
      }

      test_that("change for simple values is accurate", {
          expect_equal(change_for(1, c(1)), list(c(1)))
          expect_equal(change_for(2, c(1)), list(rep(1, 2)))
          expect_equal(change_for(5, c(1)), list(rep(1, 5)))

          ch <- lapply(seq(5, 0, -1), function(k) c(rep(25, k), rep(5, 5*(5-k))))
          expect_that(change_for(125, c(5, 25)), equals_any_order(ch))
      })

      test_that("change adds up to the correct amount, with pennies", {
          totals_match <- function(change_sets, value) {
              return( all(sapply(change_sets, sum) == value) )
          }

          # For us coins first
          us_coins <- c(1, 5, 10, 25)
          for( value in seq(1, 99) ) {
              expect_true(totals_match(change_for(value, us_coins), value),
                          info=list(value=value, coins=us_coins))
          }

          # Random Coin Sets, always include pennies to prevent null result
          for( num_tests in seq(1, 10000) ) {
              num_coins <- 1 + rpois(1, 4)
              coins <- c(1, sort(sample(2:99, num_coins, replace=FALSE)))

              for( value in seq(1, 99) ) {
                  expect_true(totals_match(change_for(value, coins), value),
                              info=list(value=value, coins=coins))
              }
          }
      })

      test_that("no repeats are generated", {
          for( num_tests in seq(1, 10000) ) {
              num_coins <- 1 + rpois(1, 4)
              coins <- c(1, sort(sample(2:99, num_coins, replace=FALSE)))

              for( value in seq(1, 99) ) {
                  changes <- change_for(value, coins)
                  expect_true(length(changes) == length(unique(changes)),
                              info=list(value=value, coins=coins))
              }
          }

      })

      test_that("empty vector produced when no change can be made", {
          expect_true(length(change_for(100, c(99))) == 0)
          expect_true(length(change_for(-1, c(1, 5, 10, 25))) == 0)
      })
    #+end_src

    #+begin_src python
      import unittest
      import random

      from change_for import change_for

      def in_any_order(x):
          sorted(x, key=str)

      class TestSimpleChange(unittest.TestCase):
          "Test that change for simple values is accurate."

          def test_single(self):
              self.assertEqual(list(change_for(1, [1])), [[1]])
              self.assertEqual(list(change_for(2, [1])), [[1, 1]])
              self.assertEqual(list(change_for(5, [1])), [[1, 1, 1, 1, 1]])

          def test_combinations(self):
              ch = [[25]*k + [5] * (5*(5-k)) for k in range(5,-1,-1)]
              self.assertEqual(in_any_order(list(change_for(125, [5, 25]))),
                               in_any_order(ch))

      class TestCorrectChange(unittest.TestCase):
          "Test that change adds up to the correct amount, assuming pennies."

          test_count = 10

          def test_us(self):
              us_coins = [1, 5, 10, 25]

              for value in range(1,100):
                  for change in change_for(value, us_coins):
                      self.assertEqual(sum(change), value)

          def test_random(self):
              for num_test in range(self.test_count):
                  num_coins = random.randint(2, 10)
                  coins = [1] + sorted(random.sample(range(2,100), num_coins))

                  for value in range(1,100):
                      for change in change_for(value, coins):
                          self.assertEqual(sum(change), value,
                                           msg = str(value) + ' <-> ' + str(coins))

      # ATTN: etc....

      class TestEmptyChange(unittest.TestCase):
          def test_unable(self):
              self.assertTrue(len(list(change_for(100, [99]))) == 0)

          def test_negative(self):
              self.assertTrue(len(list(change_for(-1, [1,5,10,25]))) == 0)


      if __name__ == '__main__':
          unittest.main()

    #+end_src

    #+begin_src clojure
      (let [us-coins      [25 10 5 1]
            sum           (fn [x] (reduce + 0 x))
            every-sum-is? (fn [v] (fn [s] (every? #(= v %) (map sum s))))]
        (fact "Correctness on a few basic examples"
          (change-for 1 [1])        => (list [1])
          (change-for 5 [1])        => (list [1 1 1 1 1])
          (change-for 125 [25 5])   => (just (for [k (range 5 -1 -1)]
                                               (vec (concat (repeat k 25)
                                                            (repeat (* 5 (- 5 k)) 5))))
                                             :in-any-order)
          (change-for 125 [25 5])   => (every-sum-is? 125)
          (change-for 128 us-coins) => (every-sum-is? 128))
        (fact "Empty sequence when no change can be made"
          (change-for 100 [99])     => empty?
          (change-for -1 us-coins)  => empty?))
    #+end_src

*** Pseudo-Code

    ~change_for(amount, coins, current):~

    1. If the amount is zero, return list containing current.
    2. Otherwise, if coins is empty or if amount negative, return empty list.
    3. Subproblem 1: pick a coin c from coins
                     change_for(amount - c, coins, current 'append' c)
    4. Subproblem 2: pick a coin c from coins
                     change_for(amount, coins - c, current)
    5. Combine: concatenate results from subproblems 1 and 2

*** Basic Implementation

    #+begin_src R
      change_for <- function(amount, coins, current=NULL) {
          assert_that(length(coins) == length(unique(coins)))

          if ( amount == 0 ) {
              return( list(current) )
          } else if ( amount < min(coins) || length(coins) == 0 ) {
              return( list() )
          } else {
              next_coin <- coins[1]
              next_amount <- amount - next_coin
              sequel <- c(current, next_coin)

              return( c(change_for(next_amount, coins, sequel),
                        change_for(amount, coins[-1], current)) )
          }
      }
      # Note: There's a subtle "bug" here.
    #+end_src

    #+begin_src python
      from itertools import chain

      def change_for(amount, coins, current=None):
          assert coins is not None, "Coin denominations required"

          coins = set(coins)
          current = [] if current is None else current

          if amount == 0:
              yield current
          elif amount < 0 or len(coins) == 0:
              return # cannot make change
          else:
              reduced_coins = coins.copy()
              next_coin = reduced_coins.pop() # removes arbitrary coin
              next_amount = amount - next_coin
              sequel = current[:]
              sequel.append(next_coin)

              ways = chain(change_for(next_amount, coins, sequel),
                           change_for(amount, reduced_coins, current))
              for change in ways:
                  yield change
    #+end_src

    #+begin_src clojure
      (defn change-for
        "Returns a lazy sequence of sets representing distinct ways to make
        change for a given monetary amount (in cents) using a set of coin
        denominations (also in cents). Each element of the returned sequence
        is a vector representing a multi-set of values in the denominations
        set."
        ([amount denominations]
         {:pre [(every? integer? denominations)]}
         ;; Sorting the denominations in decreasing order improves performance.
         (change-for amount (apply sorted-set-by > denominations) []))
        ([amount denominations current]
         {:pre [(integer? amount) (set? denominations) (coll? current)]}
         (lazy-seq
          (cond
            (zero? amount) [current]
            (or (neg? amount) (empty? denominations)) nil
            :else (let [next-denom (first denominations)
                        amount'    (- amount next-denom)
                        sequel     (conj current next-denom)]
                    (lazy-cat
                     (change-for amount' denominations sequel)
                     (change-for amount (disj denominations next-denom) current)))))))
    #+end_src

* Example: Mergesort

  Problem: Given an array of objects a_1, ..., a_n from an ordered set
  (e.g., numbers), find a permutation of the array such that a_1 <= a_2
  <= ... <= a_n.

  One divide-and-conquer approach to this problem is *mergesort*:

   + *Divide* the n-element array into two arrays of n/2 elements each.
   + *Conquer* by sorting the two subarrays recursively using ~mergesort~.
   + *Combine* the sorted subarrays by merging into a single sorted array.

  The base case here is when the subarrays have length 1.

** Utility function =merge=

   Define a procedure =merge(a, lo, mid, hi)= that takes an array a
   and indices lo <= mid < hi into a where subarrays =a[lo..mid]=
   and =a[mid+1..hi]= are assumed in sorted order and merges the
   two subarrays in place to sort a[lo..hi].

   How does the computation time of merge vary with n = hi - lo + 1?

** On to =mergeSort=

   Assume you have ~merge~.  Write ~mergeSort(a)~ that sorts ~a~
   using divide-and conquer.

   #+begin_src R
     mergeSort <- function(a, lo, hi) {
         if ( lo < hi ) { # at least two elements in subarray
             mid <- floor((lo + hi)/2)
             a <- mergeSort(a, lo, mid)
             a <- mergeSort(a, mid+1, hi)
             a <- merge(a, lo, mid, hi)
         }
         return( a )
     }
   #+end_src

** Back to =merge=

   How would you implement merge?  (Hint: Think about two piles of cards.)

   #+BEGIN_EXAMPLE
     merge(a, lo, mid, hi):
       m = mid - lo + 1
       n = hi - mid
       L[1..m] = a[lo..mid]
       L[m+1]  = infinity       # Sentinel
       R[1..n] = a[mid+1..hi]
       R[n+1]  = infinity       # Sentinel

       i = j = 1
       for k from lo upto hi inclusive:
           if L[i] <= R[j]:
               a[k] = L[i]
               i = i + 1
           else:
               a[k] = R[j]
               j = j + 1

       return a
   #+END_EXAMPLE

   What are the loop invariants in the above code?
   What do the infinities do for us here?  Are they necessary?

* Example: Multiplication

  Multiplying two complex numbers seems to require /four/ multiplications:
  \begin{equation*}
    (a + i b) (c + i d) = a c - bd + i (ad + bc),
  \end{equation*}
  but we can do it in three
  \begin{equation*}
  (a + i b) (c + i d) = a c - bd + i [(a + b)(c + d) - a c - b d].
  \end{equation*}
  This reduction of cost by 3/4 might not be a big deal for a single
  multiplication, but when we apply this strategy *recursively*
  the 3/4 gain is applied at each stage -- to significant effect.

  Now consider multipying two n-bit integers x and y, where n is a power
  of 2. Thinking in terms of their binary representations, we can write
  \begin{align*}
     x &= 2^{n/2} x_L + x_R \\
     y &= 2^{n/2} y_L + y_R,
  \end{align*}
  where $x_L$, $x_R$, etc. are n/2-bit integers.
  For example, if $x = 10110110_2$ then $x_L = 1011_2$ and $x_R = 0110_2$.

  We can apply the multiplication trick above to reduce four multiplications to three:
  \begin{equation*}
  x y = 2^n x_L y_L + x_R y_R + 2^{n/2} [(x_L + x_R)(y_L + y_R) - x_L y_L - x_R y_R].
  \end{equation*}
  The additions and multiplication by powers of 2 (shifts) are fast. The
  arbitrary multiplications are the costly operation, but we have
  reduced the problem to three multiplications of half the size. We can
  apply the same trick recursively to each of these three
  multiplications.

  #+BEGIN_EXAMPLE
  fast_multiply(x,y):
     n = max(bitwidth(x), bitwidth(y))
     if n <= NATIVE_BITS:
         return x*y

     x_L, x_R = leftmost ceiling(n/2), rightmost floor(n/2) bits of x
     y_L, y_R = leftmost ceiling(n/2), rightmost floor(n/2) bits of y

     a = fast_multiply(x_L, y_L)
     b = fast_multiply(x_R, y_R)
     c = fast_multiply(x_L + x_R, y_L + y_R)
     return (a << n) + b + (c - a - b) << n/2
  #+END_EXAMPLE

** Practical note
   On real machines, it would not make sense to recurse down to the
   single bit level. Rather, the base case would be whatever size
   supports fast native multiplications on the given machine (e.g., 16-,
   32-, or 64-bit).

** Improved computational cost
   We can estimate the number of operations $T_n$ required for this
   recursive algorithm on an n-bit number:
   \begin{equation*}
     T_n \le 3 T_{n/2} + O(n),
   \end{equation*}
   which gives us $T_n = O(n^{\log_2 3}) \approx O(n^{1.59})$. This is
   notably better than the $O(n^2)$ performance we get if we use four
   multiplications.

** Extending to matrix multiplication
   The same idea applies to matrix multiplication (called Strassen's
   algorithm). The standard blockwise matrix multiplication gives eight
   multiplications of submatrices:
   \begin{equation*}
   \mat([A,B][C,D]) \times \mat([E,F][G,H]) = \mat([AE+BG,AF+BH][CE+DG,CF+DH]).
   \end{equation*}
   We can reduce this to /seven/ multiplications with a bit of rearrangement.
   \begin{equation*}
   \mat([A,B][C,D]) \times \mat([E,F][G,H]) = \mat([M_5+M_4-M_2+M_6,M_1+M_2][M_3+M_4,M_1+M_5-M_3-M_7]),
   \end{equation*}
   where
   \begin{align*}
   M_1 &= A (F - H)   &   & M_5 = (A + D)(E + H) \\
   M_2 &= (A + B) H   &   & M_6 = (B - D)(G + H) \\
   M_3 &= (C + D) E   &   & M_7 = (A - C)(E + F) \\
   M_4 &= D (G - E).
   \end{align*}
   Now recurse. This yields $O(n^{\log_2 7})$ versus $O(n^3)$.

* Exercise: Selecting the Median

  Suppose you have a numeric vector with a very large dimension $n$
  (in the billions, say, or even more). You want to find the median
  of these values.

  What's the most direct way to do this? What are some issues
  we might face with that approach?

  + Reading everything into memory
  + Custom software
  + Want linear?

  Let's devise a divide-and-conquer method for this problem. To do this,
  we need to (at least):

  1. Identify appropriate subproblems
  2. Decide which subproblems to solve recursively and which to solve
     directly.
  3. Determine how to combine subproblem solutions to get solutions to
     larger problems.

  Number 1 is the tricky part here. First, remember that the recursive
  structure means that we will apply the same algorithm to the smaller
  problems. Second, suppose you had a median value $v$ already. What
  properties does it satisfy? What subproblems does this suggest?

  Ideas??

  We will develop pseudo-code for a function ~median(X)~ that
  computes a median of the vector ~X~ by divide-and-conquer.

  #+begin_example
  Pick a candidate value v
  Partition X into three pieces:  X_L, X_v, X_G
  #+end_example

  How many ``steps'' does this algorithm take?  How does
  this performance depend on our choice at each stage?

* Notes-Only Example: Selecting General Order Statistics
  Given a list X, we want to pick the k-th smallest element in X. We
  could sort them and pick it, but we would like a *linear*-time
  algorithm.

  =selection(X,k)=:
  1. Pick a random ``pivot'' v
  2. Partition X into X_L, X_v, X_R, values less than, equal to, and
     greater than v.
  3. Let $k' = k - size(X_L) - size(X_v)$.
  4. Then return
     \begin{equation*}
     \selection(X,k) = \begin{cases}
                          \selection(X_L, k)  & \mbox{if } k \le \size(X_L) \\
                           v                  & \mbox{if } \size(X_L) < k \le \size(X_L) + \size(X_v)\\
                          \selection(X_R, k') & \mbox{if } k > \size(X_L) + \size(X_v).
                       \end{cases}
     \end{equation*}

  The computation of the three sublists can be done in linear time (and
  in place, see homework). At each stage, we reduce the size of the list
  from $\size(X)$ to $\max(\size(X_L), \size(X_R))$.

  If we pick $v$ to split $X$ in half, we get a linear time algorithm.

  But that would involve picking the median, which is one of the order
  statistics we might be trying to compute. Ooops. Instead, to choose
  $v$ at each stage, take a (small) random sample from the list and pick
  the median of the sample. With high probability, that choice of pivot
  will be a good one. This tends to work in practice.

* Notes-Only Example: Fast Fourier Transform (FFT)
  The FFT is a critically important algorithm for computing a critically
  important mathematical transform.

  The FFT is based on a divide-and-conquer algorithm for
  *fast polynomial multiplication*, and it has other
  recursive representations as well.

  If
  \begin{align*}
    A(x) &= a_0 + a_1 x + \cdots + a_d x^d \\
    B(x) &= b_0 + b_1 x + \cdots + b_d x^d,
  \end{align*}
  then
  \begin{equation*}
    C(x) = A(x) B(x) = c_0 + c_1 x + \cdots + c_{2d} x^{2d},
  \end{equation*}
  where  $c_k = a_0 b_k + a_1 b_{k-1} + \cdots + a_k b_0$.
  The coefficients of C are the *convolutions* of the coefficients
  of A and B.

** Key ideas:
    - A d-degree polynomial is determined by its value at $d+1$ distinct
      points.
    - This gives us two equivalent but distinct *representations* of a polynomial:
      the coefficients and the values at selected points.
    - It is slow to multiply polynomials in the /coefficient representation/,
      but fast to multiply them in the /value representation/.
    - Divide and conquer gives us a fast, recursive way to evaluate
      polynomials if we use special points called the roots of unity
    - Divide and conquer gives us a fast way to move between coefficient
      and value representation, almost for free.

*** Multiplying in the value representation
    1. Pick $n \ge 2d + 1$ distinct points $x_1, \ldots, x_n$
    2. Given polynomials (value representation) $A(x_1), \ldots, A(x_n)$
       and $B(x_1), \ldots, B(x_n)$, form products
       $$
       C(x_1), \ldots, C(x_n) = A(x_1)B(x_1), \ldots, A(x_n)B(x_n)
       $$
    3. Recover coefficient representation of C

*** Divide and conquer

    We can split A (and similarly B) as follows
    \begin{equation*}
       A(x) = A_{\rm even}(x^2) + x A_{\rm odd}(x^2).
    \end{equation*}
    Then to compute any pair of values $\pm x$, we can share
    calculations -- giving a speed up.
    \begin{align*}
    A(x)  &= A_{\rm even}(x^2) + x A_{\rm odd}(x^2) \\
    A(-x) &= A_{\rm even}(x^2) - x A_{\rm odd}(x^2),
    \end{align*}
    requiring two evaluations of a smaller polynomial to compute
    both two evaluations of A.

    Using pairs $\pm x_0, \ldots, \pm x_{n/2-1}$ seems to work fine
    at the first level. But the next set of points
    is $x_0^2, \ldots, x_{n/2-1}^2$.  How can those be positive-negative
    pairs???

    The answer: use well chosen complex numbers -- /n-th roots of unity/!

    If $\omega^n = 1$, $\omega$ is an $n$th root of unity. The value
    $\omega = e^{2\pi i/n}$ is called the /principal/ $n$th root of
    unity because all the others are derived from it. The $n$th roots
    of unity satisfy two properties that are important for our purposes:

      + They come in positive-negative pairs:  $\omega^{n/2 + j} = -\omega^j$.
      + Squaring them produces (n/2)-nd roots of unity: $(\omega^2)^{n/2} = 1$.

    So if we start with the right roots of unity, our divide and conquer
    recursion keeps its special properties throughout.

    #+BEGIN_EXAMPLE
    fft(A, w):
        A is a polynomial in the coefficient representation
        w is an nth root of unity
    #+END_EXAMPLE

    Do:
    1. If w == 1, return A(1)
    2. Write $A(x) = A_{\rm even}(x^2) + x A_{\rm odd}(x^2)$
    3. Call $\texttt{fft}(A_{\rm even}, w^2)$ to evaluate A_even at even powers of w
    4. Call $\texttt{fft}(A_{\rm odd}, w^2)$ to evaluate A_odd at even powers of w
    5. for j in 0..n-1:  $A(w^j) = A_{\rm even}(w^2j) + w^j A_{\rm odd}(w^2j)$

    Return $A(w^0)$, ..., $A(w^{n-1})$

*** Interpolation

    As stated above, the divide and conquer gives us a fast, recursive
    method to multiply value representations. But amazingly, it gives us
    a fast way to convert to the coefficient representation as well.

    It turns out that when
        #+BEGIN_EXAMPLE
        values = fft(coefficients, w)
        #+END_EXAMPLE

    we also have

        #+BEGIN_EXAMPLE
        coefficients = fft(values, w^{-1})/n.
        #+END_EXAMPLE

    Polynomial multiplication thus looks like:

    1. Apply fft to convert from coefficient to value representation
    2. Multiply value representations
    3. Apply (inverse) fft to convert back to coefficient representation

** Generalizing

   It is useful to express this in matrix-vector terms.
   For instance, a polynomial A can be written as
   \begin{equation*}
   \left[\begin{array}{c}A(x_0)\\A(x_1)\\\vdots\\A(x_{n-1})\end{array}\right] =
     \left[\begin{array}{ccccc}
            1 & x_0 & x_0^2 & \cdots & x_0^{n-1} \\
            1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
              &     & \vdots &       & \\

            1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-1} \\
            \end{array}\right] \,
     \left[\begin{array}{c}a_0\\a_1\\\vdots\\a_{n-1}\end{array}\right],
   \end{equation*}
   with the matrix -- call it $M(x)$ -- known as the /Vandermonde/ matrix.

   As long as $x_0,\ldots,x_{n-1}$ are distinct, $M(x)$ is invertible.

   Evaluation is multiplication by $M$; interpolation is multiplication
   by $M^{-1}$.

   The FFT is just multiplication by $M(\omega)$ for an nth root of unity.
   The inverse FFT is obtained from the fact that $M^{-1}(\omega) = M(\omega^{-1})/n$.

   Our general FFT uses divide-and-conquer to make this multiplication fast,
   similar to what we did above with the polynomials:

   #+BEGIN_EXAMPLE
   fft(a, w):

        Input:  a = (a_0, ..., a_{n-1}) is an array where n is a power of 2
                w is a primitive nth root of unity
        Output: M_n(w) a

   if w == 1:
       return a
   (s_0, ..., s_{n/2 - 1}) = fft((a0, a_2, ..., a_{n-2}), w^2)
   (u_0, ..., u_{n/2 - 1}) = fft((a1, a_3, ..., a_{n-1}), w^2)

   for j from 0 to n/2 - 1:
       r_j = s_j + w^j u_j
       r_{j + n/2} = s_j - w^j u_j

   return (r_0, ..., r_{n-1})
   #+END_EXAMPLE
